<!DOCTYPE html>
<html lang="en">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">




<title>Scraping all the texts of Luxun(鲁迅) from the Internet using Python (用Python爬取《鲁迅全集》) | 不辍弦歌</title>

<link rel="stylesheet" href="https://ge-chunyu.github.io//css/styles.css">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" 
integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
<script src="https://code.jquery.com/jquery-3.3.1.js" integrity="sha256-2Kok7MbOyxpgUVvAk/HJ2jigOSYS2auK4Pfzbm7uH60=" crossorigin="anonymous"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/zenburn.min.css" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script src="https://ge-chunyu.github.io//js/highlight.js"></script>



<link type="text/css" rel="stylesheet" href="https://cdn.jsdelivr.net/jquery.jssocials/1.4.0/jssocials.css" />
<link type="text/css" rel="stylesheet" href="https://cdn.jsdelivr.net/jquery.jssocials/1.4.0/jssocials-theme-minima.css" />
<script type="text/javascript" src="https://ge-chunyu.github.io//js/jssocials.js"></script>




<div class="container">
    <nav class="navbar level">
      <div class="navbar-brand">
          <a class="nav-item" href="https://ge-chunyu.github.io/"><h1 class="title is-3">不辍弦歌</h1></a>
      </div>           
      <div class="navbar-menu has-text-centered is-active">
          <div class="navbar-end is-centered">
              
                <a href="https://ge-chunyu.github.io/about" rel="me">
                  <span class="icon">
                    <i class="fas fa-info"></i>
                  </span>
                </a>
              
                <a href="https://github.com/ge-chunyu/" rel="me">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                </a>
              
                <a href="mailto:gechunyu92@hotmail.com" rel="me">
                  <span class="icon">
                    <i class="fas fa-envelope"></i>
                  </span>
                </a>
              
                <a href="https://ge-chunyu.github.io/" rel="me">
                  <span class="icon">
                    <i class=""></i>
                  </span>
                </a>
              
                <a href="https://ge-chunyu.github.io/" rel="me">
                  <span class="icon">
                    <i class=""></i>
                  </span>
                </a>
              
           </div>
      </div>
    </nav>
  </div>

<div class="container">
  <h2 class="subtitle is-6">October 12, 2019</h2>
  <h1 class="subtitle is-size-4-mobile is-size-3-desktop">Scraping all the texts of Luxun(鲁迅) from the Internet using Python (用Python爬取《鲁迅全集》)</h1>
  <div class="content">
    

<p>I want to do some text mining practices on the texts of <em>Luxun(鲁迅)</em>, a great Chinese writer. The first step is to get all the texts by <em>Luxun</em>, and I have no time typing all the texts word by word. So I decided to srape the texts from an online source.</p>

<h2 id="source-of-the-texts">Source of the texts</h2>

<p>The texts of <em>Luxun</em> are scraped from <a href="http://www.ziyexing.com/">子夜星网</a>. As it claimed, it contains all the texts in the <em>Complete works of Luxun(鲁迅全集)</em>. I checked it, and so it did.</p>

<h2 id="get-the-urls-and-titles-of-all-the-articles">Get the urls and titles of all the articles</h2>

<p>The process starts at getting the contents and the urls of the text of Luxun from the parent url <code>http://www.ziyexing.com/luxun/</code>. To access all the urls, I constructed a regular expression and selected all <code>a</code> nodes that share the pattern.</p>

<pre><code class="language-python">homepage_res = requests.get(&quot;www.ziyexing.com/luxun/&quot;)
homepage_soup = BeautifulSoup(res.text, &quot;html.parser&quot;)
href_re = re.compile(r&quot;luxun_\w+_\w+_\d+.htm&quot;)
hrefs = homepage_soup.find_all(&quot;a&quot;, {&quot;href&quot;:href_re})
</code></pre>

<p>It is found that although the regular expression covers most of the patterns, some urls are idiosyncratic and do not conform to the regex. I constructed another regex.</p>

<pre><code class="language-python">others_re = re.compile(r&quot;(zhunfengyuetan)|(gushixinbian)|(gujixubaji)|(zgxssl)|(luxun_shici)\w+&quot;)
other_hrefs = homepage_soup.find_all(&quot;a&quot;, {&quot;href&quot;:others_re})
</code></pre>

<p>It is, of course, idiosyncratic, but effective. To use two regexes, I have got all the urls.</p>

<pre><code class="language-python">links = [href.attrs[&quot;href&quot;] for href in hrefs]
other_links = [line.attrs[&quot;href&quot;] for line in other_hrefs]
</code></pre>

<p>The title of each url can also be accessed in the <code>a</code> nodes. It can easily accessed using <code>a.text</code>, yet another problem appeared. The most notorious problem in dealing with non-Latin alphabet languages, especially Chinese, is the problem of encoding. When applying <code>a.text</code>, the characters did not show normally. I am fortunately enough to have recently learnt that the encoding of an <code>html</code> page can be seen from the <code>header</code> of the page. I checked it and found that the page is encoded using <code>gb2312</code>. To make the texts return to normal requires encoding in <code>Latin1</code> and subsequently decoding in <code>gb2312</code>. <code>gbk</code>, as a superset of <code>gb2312</code>, works better in decoding.</p>

<pre><code class="language-python">titles = [href.text.encode(&quot;latin1&quot;).decode(&quot;gbk&quot;) for href in hrefs]
other_titles = [line.text.encode(&quot;latin1&quot;).decode(&quot;gbk&quot;) for line in other_hrefs]
</code></pre>

<p>Have got all the urls and corresponding titles, I can proceed to the next step, to scrape all the articles. A brief inspection of the article page show that all the contents of the article are embedded in the <code>p</code> node with property <code>line-height: 150%</code>. A further inspection of other pages show that the <code>line-height</code> can also be <code>130%</code>. So another regex is needed here.</p>

<pre><code>ps_re = re.compile(r&quot;line-height: 1\d0%&quot;)
</code></pre>

<h2 id="get-the-texts">Get the texts</h2>

<p>Put all the pieces together, I wrote several functions to make the process modular and easy to understand.</p>

<p>The <code>get_soup</code> function accesses the given <code>url</code> and returns the <code>BeautifulSoup</code> object.</p>

<pre><code class="language-python">def get_soup(base_url, url):
    res = requests.get(base_url + url)
    soup = BeautifulSoup(res.text, &quot;html.parser&quot;)
    return soup
</code></pre>

<p>The <code>get_ps</code> function accepts the <code>soup</code> object and outputs the <code>p</code> nodes, which contain the texts.</p>

<pre><code class="language-python">def get_ps(soup):
    ps_re = re.compile(r&quot;line-height: 1\d0%&quot;)
    ps = soup.find_all(&quot;p&quot;, {&quot;style&quot;:ps_re})
    return ps
</code></pre>

<p>The <code>clean_text</code> function accepts the <code>p</code> nodes and outputs the cleaned text.</p>

<pre><code class="language-python">def clean_text(texts):
    texts_decoded = [text.encode(&quot;latin1&quot;, &quot;ignore&quot;).decode(&quot;gbk&quot;, &quot;ignore&quot;) for text in texts]
    texts_decoded = [text.strip() for text in texts_decoded]
    cleaned_texts = [text for text in texts_decoded if text != &quot;&quot;]
    return cleaned_texts
</code></pre>

<p>The <code>write_text</code> function writes the text data in <code>txt</code> fromat in a file named after the title of the article.</p>

<pre><code class="language-python">def write_text(clean_text, titles, n):
    with open(&quot;luxun/&quot; + titles[n].strip() + &quot;.txt&quot;, &quot;w&quot;, encoding=&quot;utf8&quot;) as file:
        file.write(&quot;\n&quot;.join(clean_text))
</code></pre>

<p>To wrap all these function together, I wrote a <code>main</code> function which do all these stuff at once.</p>

<pre><code class="language-python">def main(links, titles):
    for i in range(len(links)):
        soup = get_soup(base_url, links[i])
        ps = get_ps(soup)
        texts = ps[0].text.split(&quot;\n&quot;)
        cleaned_texts = clean_text(texts)
        write_text(cleaned_texts, titles, i)
        time.sleep(3)
</code></pre>

<p>To avoid too much traffic for the site, I used the <code>time.sleep</code> function to pause for three seconds between urls.</p>

<p>Running the <code>main()</code> function, and I got all the articles posted on <a href="http://www.ziyexing.com/">子夜星网</a> by <em>Luxun</em> in one folder.</p>

<h2 id="key-points"><strong>Key points</strong></h2>

<p>There are some traps in this toy project, some of which are interesting. I list some key points below.</p>

<ul>
<li>Use a regex to capture the pattern of the desired urls;</li>
<li>If a regex can not exhaust the pattern, write another one;</li>
<li>Look for encoding schemes in the <code>header</code> of html files;</li>
<li><code>Latin-1</code> works great for Chinese characters!

<ul>
<li>Use <code>text.encode('latin1').decode('gbk')</code>.</li>
</ul></li>
</ul>

  </div>
</div>
<div class="container has-text-centered">
    
    <aside><div id="share"></div></aside>
    <script type="text/javascript">
        $("#share").jsSocials({
            showLabel: false,
            showCount: false,
            shares: ["email", "twitter", "facebook", "googleplus", "linkedin", "pinterest", "stumbleupon", "whatsapp"]
        });
    </script>
    
</div>

<div class="container has-text-centered">
  
</div>
<section class="section">
  <div class="container has-text-centered">
    <p>&copy; <a href="https://github.com/ge-chunyu">Chunyu Ge</a> 2019-2020</p>
  </div>
</section>


