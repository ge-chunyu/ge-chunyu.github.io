<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.58.3" />


<title>Scraping all the texts of Luxun(鲁迅) from the Internet using Python (用Python爬取《鲁迅全集》) - 不辍弦歌</title>
<meta property="og:title" content="Scraping all the texts of Luxun(鲁迅) from the Internet using Python (用Python爬取《鲁迅全集》) - 不辍弦歌">




  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href=""></a></li>
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/about/">About Chunyu</a></li>
    
    <li><a href="https://github.com/ge-chunyu/">GitHub</a></li>
    
    <li><a href="/">不辍弦歌</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    

    <h1 class="article-title">Scraping all the texts of Luxun(鲁迅) from the Internet using Python (用Python爬取《鲁迅全集》)</h1>

    

    <div class="article-content">
      

<p>I want to do some text mining practices on the texts of <em>Luxun(鲁迅)</em>, a great Chinese writer. The first step is to get all the texts by <em>Luxun</em>, and I have no time typing all the texts word by word. So I decided to srape the texts from an online source.</p>

<h2 id="source-of-the-texts">Source of the texts</h2>

<p>The texts of <em>Luxun</em> are scraped from <a href="http://www.ziyexing.com/">子夜星网</a>. As it claimed, it contains all the texts in the <em>Complete works of Luxun(鲁迅全集)</em>. I checked it, and so it did.</p>

<h2 id="get-the-urls-and-titles-of-all-the-articles">Get the urls and titles of all the articles</h2>

<p>The process starts at getting the contents and the urls of the text of Luxun from the parent url <code>http://www.ziyexing.com/luxun/</code>. To access all the urls, I constructed a regular expression and selected all <code>a</code> nodes that share the pattern.</p>

<pre><code class="language-python">homepage_res = requests.get(&quot;www.ziyexing.com/luxun/&quot;)
homepage_soup = BeautifulSoup(res.text, &quot;html.parser&quot;)
href_re = re.compile(r&quot;luxun_\w+_\w+_\d+.htm&quot;)
hrefs = homepage_soup.find_all(&quot;a&quot;, {&quot;href&quot;:href_re})
</code></pre>

<p>It is found that although the regular expression covers most of the patterns, some urls are idiosyncratic and do not conform to the regex. I constructed another regex.</p>

<pre><code class="language-python">others_re = re.compile(r&quot;(zhunfengyuetan)|(gushixinbian)|(gujixubaji)|(zgxssl)|(luxun_shici)\w+&quot;)
other_hrefs = homepage_soup.find_all(&quot;a&quot;, {&quot;href&quot;:others_re})
</code></pre>

<p>It is, of course, idiosyncratic, but effective. To use two regexes, I have got all the urls.</p>

<pre><code class="language-python">links = [href.attrs[&quot;href&quot;] for href in hrefs]
other_links = [line.attrs[&quot;href&quot;] for line in other_hrefs]
</code></pre>

<p>The title of each url can also be accessed in the <code>a</code> nodes. It can easily accessed using <code>a.text</code>, yet another problem appeared. The most notorious problem in dealing with non-Latin alphabet languages, especially Chinese, is the problem of encoding. When applying <code>a.text</code>, the characters did not show normally. I am fortunately enough to have recently learnt that the encoding of an <code>html</code> page can be seen from the <code>header</code> of the page. I checked it and found that the page is encoded using <code>gb2312</code>. To make the texts return to normal requires encoding in <code>Latin1</code> and subsequently decoding in <code>gb2312</code>. <code>gbk</code>, as a superset of <code>gb2312</code>, works better in decoding.</p>

<pre><code class="language-python">titles = [href.text.encode(&quot;latin1&quot;).decode(&quot;gbk&quot;) for href in hrefs]
other_titles = [line.text.encode(&quot;latin1&quot;).decode(&quot;gbk&quot;) for line in other_hrefs]
</code></pre>

<p>Have got all the urls and corresponding titles, I can proceed to the next step, to scrape all the articles. A brief inspection of the article page show that all the contents of the article are embedded in the <code>p</code> node with property <code>line-height: 150%</code>. A further inspection of other pages show that the <code>line-height</code> can also be <code>130%</code>. So another regex is needed here.</p>

<pre><code>ps_re = re.compile(r&quot;line-height: 1\d0%&quot;)
</code></pre>

<h2 id="get-the-texts">Get the texts</h2>

<p>Put all the pieces together, I wrote several functions to make the process modular and easy to understand.</p>

<p>The <code>get_soup</code> function accesses the given <code>url</code> and returns the <code>BeautifulSoup</code> object.</p>

<pre><code class="language-python">def get_soup(base_url, url):
    res = requests.get(base_url + url)
    soup = BeautifulSoup(res.text, &quot;html.parser&quot;)
    return soup
</code></pre>

<p>The <code>get_ps</code> function accepts the <code>soup</code> object and outputs the <code>p</code> nodes, which contain the texts.</p>

<pre><code class="language-python">def get_ps(soup):
    ps_re = re.compile(r&quot;line-height: 1\d0%&quot;)
    ps = soup.find_all(&quot;p&quot;, {&quot;style&quot;:ps_re})
    return ps
</code></pre>

<p>The <code>clean_text</code> function accepts the <code>p</code> nodes and outputs the cleaned text.</p>

<pre><code class="language-python">def clean_text(texts):
    texts_decoded = [text.encode(&quot;latin1&quot;, &quot;ignore&quot;).decode(&quot;gbk&quot;, &quot;ignore&quot;) for text in texts]
    texts_decoded = [text.strip() for text in texts_decoded]
    cleaned_texts = [text for text in texts_decoded if text != &quot;&quot;]
    return cleaned_texts
</code></pre>

<p>The <code>write_text</code> function writes the text data in <code>txt</code> fromat in a file named after the title of the article.</p>

<pre><code class="language-python">def write_text(clean_text, titles, n):
    with open(&quot;luxun/&quot; + titles[n].strip() + &quot;.txt&quot;, &quot;w&quot;, encoding=&quot;utf8&quot;) as file:
        file.write(&quot;\n&quot;.join(clean_text))
</code></pre>

<p>To wrap all these function together, I wrote a <code>main</code> function which do all these stuff at once.</p>

<pre><code class="language-python">def main(links, titles):
    for i in range(len(links)):
        soup = get_soup(base_url, links[i])
        ps = get_ps(soup)
        texts = ps[0].text.split(&quot;\n&quot;)
        cleaned_texts = clean_text(texts)
        write_text(cleaned_texts, titles, i)
        time.sleep(3)
</code></pre>

<p>To avoid too much traffic for the site, I used the <code>time.sleep</code> function to pause for three seconds between urls.</p>

<p>Running the <code>main()</code> function, and I got all the articles posted on <a href="http://www.ziyexing.com/">子夜星网</a> by <em>Luxun</em> in one folder.</p>

<h2 id="key-points"><strong>Key points</strong></h2>

<p>There are some traps in this toy project, some of which are interesting. I list some key points below.</p>

<ul>
<li>Use a regex to capture the pattern of the desired urls;</li>
<li>If a regex can not exhaust the pattern, write another one;</li>
<li>Look for encoding schemes in the <code>header</code> of html files;</li>
<li><code>Latin-1</code> works great for Chinese characters!

<ul>
<li>Use <code>text.encode('latin1').decode('gbk')</code>.</li>
</ul></li>
</ul>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

