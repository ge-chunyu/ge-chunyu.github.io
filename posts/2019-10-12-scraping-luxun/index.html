<!DOCTYPE html>
<html lang="en-US">
<head>
    <meta charset="utf-8">
    
    <title>Scraping all the texts of Luxun(È≤ÅËøÖ) from the Internet using Python (Áî®PythonÁà¨Âèñ„ÄäÈ≤ÅËøÖÂÖ®ÈõÜ„Äã) | ‰∏çËæçÂº¶Ê≠å</title>
    <meta name="description" content="">
    <meta name="author" content="">
    
    <link rel="apple-touch-icon" sizes="180x180" href=https://ge-chunyu.github.io/apple-touch-icon.png>
    <link rel="icon" type="image/png" sizes="32x32" href=https://ge-chunyu.github.io/favicon-32x32.png>
    <link rel="icon" type="image/png" sizes="16x16" href=https://ge-chunyu.github.io/favicon-16x16.png>
    <link rel="manifest" href=https://ge-chunyu.github.io/site.webmanifest>
    <link rel="mask-icon" href=https://ge-chunyu.github.io/safari-pinned-tab.svg color="#00416a">
    <meta name="msapplication-TileColor" content="#00aba9">
    <meta name="theme-color" content="#ffffff">
    
    <link rel="me" href="mailto:gechunyu92@hotmail.com">
    <link rel="me" href="https://github.com/ge-chunyu">
    
    
    
    <link rel="authorization_endpoint" href="https://indieauth.com/auth">
    
    <link rel="stylesheet" href=https://ge-chunyu.github.io/css/fonts.css />
    <link rel="stylesheet" href=https://ge-chunyu.github.io/css/style.css />
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
</head>

<body>
    <div id="sitelogo">
        <a class="glyph" alt="Home" href="https://ge-chunyu.github.io/"><img src=https://ge-chunyu.github.io/images/site-logo.svg alt="Site Logo" height="64px" width="64px"></a>
    </div>
    <header>
        <nav>
    
    <div id="page-nav">
        <div class="page-nav-item">
            <a href="https://ge-chunyu.github.io/">Home</a>
        </div>
        
            <div class="page-nav-item">
                <a href="/about/">
                    
                    <span>About</span>
                </a>
            </div>
        
            <div class="page-nav-item">
                <a href="/">
                    
                    <span>‰∏çËæçÂº¶Ê≠å</span>
                </a>
            </div>
        
    </div>
</nav>
        
    </header>




    

<div id="content">
    <article class="h-entry">
        <header>
            <h1 class="post-title p-name">Scraping all the texts of Luxun(È≤ÅËøÖ) from the Internet using Python (Áî®PythonÁà¨Âèñ„ÄäÈ≤ÅËøÖÂÖ®ÈõÜ„Äã)</h1>
            
            <p class="post-date">Posted on
                <time class="dt-published" datetime="2019-10-12T00:00:00Z">
                12 October, 2019 at 00:00 UTC
                </time>  by <a href="https://ge-chunyu.github.io/" class="p-author h-card" rel="author">Chunyu Ge ËëõÊ∑≥ÂÆá</a>
            </p>
            
            
        </header>
        <section class="content e-content">
            

<p>I want to do some text mining practices on the texts of <em>Luxun(È≤ÅËøÖ)</em>, a great Chinese writer. The first step is to get all the texts by <em>Luxun</em>, and I have no time typing all the texts word by word. So I decided to srape the texts from an online source.</p>

<h2 id="source-of-the-texts">Source of the texts</h2>

<p>The texts of <em>Luxun</em> are scraped from <a href="http://www.ziyexing.com/">Â≠êÂ§úÊòüÁΩë</a>. As it claimed, it contains all the texts in the <em>Complete works of Luxun(È≤ÅËøÖÂÖ®ÈõÜ)</em>. I checked it, and so it did.</p>

<h2 id="get-the-urls-and-titles-of-all-the-articles">Get the urls and titles of all the articles</h2>

<p>The process starts at getting the contents and the urls of the text of Luxun from the parent url <code>http://www.ziyexing.com/luxun/</code>. To access all the urls, I constructed a regular expression and selected all <code>a</code> nodes that share the pattern.</p>

<pre><code class="language-python">homepage_res = requests.get(&quot;www.ziyexing.com/luxun/&quot;)
homepage_soup = BeautifulSoup(res.text, &quot;html.parser&quot;)
href_re = re.compile(r&quot;luxun_\w+_\w+_\d+.htm&quot;)
hrefs = homepage_soup.find_all(&quot;a&quot;, {&quot;href&quot;:href_re})
</code></pre>

<p>It is found that although the regular expression covers most of the patterns, some urls are idiosyncratic and do not conform to the regex. I constructed another regex.</p>

<pre><code class="language-python">others_re = re.compile(r&quot;(zhunfengyuetan)|(gushixinbian)|(gujixubaji)|(zgxssl)|(luxun_shici)\w+&quot;)
other_hrefs = homepage_soup.find_all(&quot;a&quot;, {&quot;href&quot;:others_re})
</code></pre>

<p>It is, of course, idiosyncratic, but effective. To use two regexes, I have got all the urls.</p>

<pre><code class="language-python">links = [href.attrs[&quot;href&quot;] for href in hrefs]
other_links = [line.attrs[&quot;href&quot;] for line in other_hrefs]
</code></pre>

<p>The title of each url can also be accessed in the <code>a</code> nodes. It can easily accessed using <code>a.text</code>, yet another problem appeared. The most notorious problem in dealing with non-Latin alphabet languages, especially Chinese, is the problem of encoding. When applying <code>a.text</code>, the characters did not show normally. I am fortunately enough to have recently learnt that the encoding of an <code>html</code> page can be seen from the <code>header</code> of the page. I checked it and found that the page is encoded using <code>gb2312</code>. To make the texts return to normal requires encoding in <code>Latin1</code> and subsequently decoding in <code>gb2312</code>. <code>gbk</code>, as a superset of <code>gb2312</code>, works better in decoding.</p>

<pre><code class="language-python">titles = [href.text.encode(&quot;latin1&quot;).decode(&quot;gbk&quot;) for href in hrefs]
other_titles = [line.text.encode(&quot;latin1&quot;).decode(&quot;gbk&quot;) for line in other_hrefs]
</code></pre>

<p>Have got all the urls and corresponding titles, I can proceed to the next step, to scrape all the articles. A brief inspection of the article page show that all the contents of the article are embedded in the <code>p</code> node with property <code>line-height: 150%</code>. A further inspection of other pages show that the <code>line-height</code> can also be <code>130%</code>. So another regex is needed here.</p>

<pre><code>ps_re = re.compile(r&quot;line-height: 1\d0%&quot;)
</code></pre>

<h2 id="get-the-texts">Get the texts</h2>

<p>Put all the pieces together, I wrote several functions to make the process modular and easy to understand.</p>

<p>The <code>get_soup</code> function accesses the given <code>url</code> and returns the <code>BeautifulSoup</code> object.</p>

<pre><code class="language-python">def get_soup(base_url, url):
    res = requests.get(base_url + url)
    soup = BeautifulSoup(res.text, &quot;html.parser&quot;)
    return soup
</code></pre>

<p>The <code>get_ps</code> function accepts the <code>soup</code> object and outputs the <code>p</code> nodes, which contain the texts.</p>

<pre><code class="language-python">def get_ps(soup):
    ps_re = re.compile(r&quot;line-height: 1\d0%&quot;)
    ps = soup.find_all(&quot;p&quot;, {&quot;style&quot;:ps_re})
    return ps
</code></pre>

<p>The <code>clean_text</code> function accepts the <code>p</code> nodes and outputs the cleaned text.</p>

<pre><code class="language-python">def clean_text(texts):
    texts_decoded = [text.encode(&quot;latin1&quot;, &quot;ignore&quot;).decode(&quot;gbk&quot;, &quot;ignore&quot;) for text in texts]
    texts_decoded = [text.strip() for text in texts_decoded]
    cleaned_texts = [text for text in texts_decoded if text != &quot;&quot;]
    return cleaned_texts
</code></pre>

<p>The <code>write_text</code> function writes the text data in <code>txt</code> fromat in a file named after the title of the article.</p>

<pre><code class="language-python">def write_text(clean_text, titles, n):
    with open(&quot;luxun/&quot; + titles[n].strip() + &quot;.txt&quot;, &quot;w&quot;, encoding=&quot;utf8&quot;) as file:
        file.write(&quot;\n&quot;.join(clean_text))
</code></pre>

<p>To wrap all these function together, I wrote a <code>main</code> function which do all these stuff at once.</p>

<pre><code class="language-python">def main(links, titles):
    for i in range(len(links)):
        soup = get_soup(base_url, links[i])
        ps = get_ps(soup)
        texts = ps[0].text.split(&quot;\n&quot;)
        cleaned_texts = clean_text(texts)
        write_text(cleaned_texts, titles, i)
        time.sleep(3)
</code></pre>

<p>To avoid too much traffic for the site, I used the <code>time.sleep</code> function to pause for three seconds between urls.</p>

<p>Running the <code>main()</code> function, and I got all the articles posted on <a href="http://www.ziyexing.com/">Â≠êÂ§úÊòüÁΩë</a> by <em>Luxun</em> in one folder.</p>

<h2 id="key-points"><strong>Key points</strong></h2>

<p>There are some traps in this toy project, some of which are interesting. I list some key points below.</p>

<ul>
<li>Use a regex to capture the pattern of the desired urls;</li>
<li>If a regex can not exhaust the pattern, write another one;</li>
<li>Look for encoding schemes in the <code>header</code> of html files;</li>
<li><code>Latin-1</code> works great for Chinese characters!

<ul>
<li>Use <code>text.encode('latin1').decode('gbk')</code>.</li>
</ul></li>
</ul>

        </section>
        <footer>
            <a class="permalink u-url" href="https://ge-chunyu.github.io/posts/2019-10-12-scraping-luxun/">üîó</a>
            
        </footer>
    </article>
</div>

    
    <div class="h-card">
      <img class="u-photo" src="https://ge-chunyu.github.io/images/site-logo.svg" />
      <div class="card-content">
        <h2 class="card-name"><a class="p-name u-url" href="https://ge-chunyu.github.io/" rel="me">Chunyu Ge ËëõÊ∑≥ÂÆá</a></h2>
        <p class="card-subhead">
          <span class="p-locality">Beijing</span>,
          <span class="p-country-name">China</span><br />
          <a class="u-email" href="mailto:gechunyu92@hotmail.com">Email me</a>
        </p>
      </div>
      <p class="p-note">A short description, a few sentences describing the author. Set the &#39;ShowBio&#39; parameter to false to hide this.</p>
    </div>
    
    <div id="footer">
      
      <nav id="article-skip">
        <div class="next">
          
          <a alt="Newer article" href="https://ge-chunyu.github.io/posts/2019-10-customizing-pdf/">&larr; Newer</a>
          
        </div>
        <div class="top">
          <a alt="Top of page" href="#">Top</a>
        </div>
        <div class="prev">
          
          <a alt="Older article" href="https://ge-chunyu.github.io/posts/2019-10-11-sublime-pandoc/">Older &rarr;</a>
          
        </div>
      </nav>
      
      <aside id="social">
  <div id="social-icons">
    
    <div class="icon-24x24">
      <a class="glyph" alt="Email me" href="mailto:gechunyu92@hotmail.com"><img src=https://ge-chunyu.github.io/icons/envelope.svg height="24px" width="24px"></a>
    </div>
    
    
    
    
    <div class="icon-24x24">
      <a class="glyph" alt="GitHub profile" href="https://github.com/ge-chunyu"><img src=https://ge-chunyu.github.io/icons/github.svg height="24px" width="24px"></a>
    </div>
    
    
    
    
    
    
    
    
    
    
    
  </div>
</aside>

      
      <p class="copyright">
          Copyright ¬© 2019, Chunyu Ge
      </p>
      
    </div>


</body>
</html>
